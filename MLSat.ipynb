{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Moshe's idea:\n",
    "1. Start with a large Boolean formula f (say, industrial SAT benchmark).\n",
    "2. Generate many samples satisfying f or \\neg f.\n",
    "(Using, for example, [https://www.nature.com/articles/s42256-018-0002-3](https://www.nature.com/articles/s42256-018-0002-3))\n",
    "3. Train a DNN on this labeled sample.\n",
    "4. Check whether it is a good apprcximation of f.\n",
    "----------\n",
    "To use this notebook, please install pysat: https://pysathq.github.io/installation.html\n",
    "Perhaps the simplest way is with the following command: pip install python-sat[pblib,aiger]\n",
    "(pysat might not work on Windows)\n",
    "'''\n",
    "\n",
    "\n",
    "# This is just a warm-up cell, to see how pysat works\n",
    "\n",
    "# some examples of pysat usage at: https://github.com/pysathq/pysat/blob/master/examples/usage.py\n",
    "from pysat.solvers import Glucose3\n",
    "g = Glucose3()\n",
    "g.add_clause([-1, 2])\n",
    "g.add_clause([-2, 3])\n",
    "\n",
    "\n",
    "for m in g.enum_models():  # implemented as a generator\n",
    "    print(m)\n",
    "g.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to use this cell, extract bw_large.d.cnf from instances/blocksworld.tar.gz\n",
    "# blocksworld is from satlib at https://www.cs.ubc.ca/~hoos/SATLIB/Benchmarks/SAT/PLANNING/BlocksWorld/blocksworld.tar.gz\n",
    "# satlib website: https://www.cs.ubc.ca/~hoos/SATLIB/benchm.html\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "from pysat.solvers import Glucose3\n",
    "from pysat.formula import CNF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "\n",
    "\n",
    "def generate_dataset(cnf_file):\n",
    "    '''\n",
    "    Receives a boolean formula in CNF format and does step 2 of the methodology:\n",
    "    2. Generate many samples satisfying f or \\neg f.\n",
    "    \n",
    "    Particularly, it enumerates all satisfying samples and the same number of unsat samples.\n",
    "    Unsat samples are generated by flipping one bit of a sat sample.\n",
    "    '''\n",
    "\n",
    "    formula = CNF(from_file=cnf_file)\n",
    "\n",
    "    num_vars = formula.nv\n",
    "    sat_list = []\n",
    "    unsat_list = []\n",
    "\n",
    "    with Glucose3(bootstrap_with=formula) as solver:\n",
    "\n",
    "        # for each positive (sat) instance, flip a literal to generate a negative (unsat) instance\n",
    "\n",
    "        # transforming each sat instance into tuple eases the generation of negative instances\n",
    "        sat_list = [m for m in solver.enum_models()] #enum_models returns a generator and not a list\n",
    "        \n",
    "        #print(sat_list[0])\n",
    "        sat_set = set([tuple(s) for s in sat_list]) # much quicker to verify an existing instance\n",
    "\n",
    "        for assignment in sat_list:\n",
    "            for i, literal in enumerate(assignment):\n",
    "                tentative_unsat = list(assignment)\n",
    "                tentative_unsat[i] = -tentative_unsat[i] #negating one literal\n",
    "                if tuple(tentative_unsat) not in sat_set:\n",
    "                    unsat_list.append(tentative_unsat)\n",
    "                    break # goes on to next assignment\n",
    "                #print(f'negated {i}-th')\n",
    "\n",
    "        #for num, m in enumerate(solver.enum_models()):  \n",
    "        #print(num)\n",
    "    #print(len(sat_list))\n",
    "    #print(len(unsat_list))\n",
    "    \n",
    "    # appends 1 to sat samples, 0 to unsat samples\n",
    "    for sat in sat_list:\n",
    "        sat.append(1)\n",
    "    for unsat in unsat_list:\n",
    "        unsat.append(0)\n",
    "    \n",
    "    # concats and shuffles the two lists\n",
    "    all_data = sat_list + unsat_list\n",
    "    #random.seed(2) # uncomment to debug (otherwise each shuffle will give a different array)\n",
    "    random.shuffle(all_data)\n",
    "    \n",
    "    # columns = [x1, x2, ..., xn, f]\n",
    "    data_colnames = [f'x{i}' for i in range(1, len(all_data[0]))]\n",
    "    df = pd.DataFrame(all_data, columns = data_colnames + ['f'])\n",
    "    \n",
    "    # replaces negatives by 0 and positives by 1\n",
    "    df.mask(df < 0, 0, inplace=True)\n",
    "    df.mask(df > 0, 1, inplace=True)\n",
    "    \n",
    "    # breaks into input features & label\n",
    "    data_x = df.drop('f', axis=1)\n",
    "    data_y = df['f']\n",
    "\n",
    "    return data_x, data_y\n",
    "\n",
    "def run_model(model, data_x, data_y, splitter=StratifiedKFold(n_splits=5)):\n",
    "    '''\n",
    "    Runs a machine learning model in the dataset\n",
    "    '''\n",
    "    \n",
    "    # prints the header\n",
    "    print('#instances\\tprec\\tacc')\n",
    "\n",
    "    # trains the model for each split (fold)\n",
    "    for train_index, test_index in splitter.split(data_x, data_y):\n",
    "        # splits the data frames in test and train\n",
    "        train_x, train_y = data_x.iloc[train_index], data_y.iloc[train_index]\n",
    "        test_x, test_y = data_x.iloc[test_index], data_y.iloc[test_index]\n",
    "        \n",
    "        model.fit(train_x, train_y)\n",
    "\n",
    "        # obtains the predictions on the test set\n",
    "        predictions = model.predict(test_x)\n",
    "\n",
    "        # calculates and reports some metrics\n",
    "        acc = accuracy_score(predictions, test_y)\n",
    "        prec = precision_score(predictions, test_y, average='macro')\n",
    "        print('{}\\t\\t{:.3f}\\t{:.3f}'.format(len(test_y), prec, acc))\n",
    " \n",
    "\n",
    "\n",
    "data_x, data_y = generate_dataset('instances/bw_large.d.cnf')\n",
    "\n",
    "print('Decision tree')\n",
    "learner = DecisionTreeClassifier()\n",
    "run_model(learner, data_x, data_y ) #default is stratified 5-fold cross validati\n",
    "#run_model(learner, data_x, data_y, StratifiedShuffleSplit(n_splits=1, test_size=0.10)) # test with holdout of 10% for test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
